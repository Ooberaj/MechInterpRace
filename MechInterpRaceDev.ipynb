{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "jrPaBT-ib8I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Key reference used: https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=rIsrKr2QzpBV",
        "%pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "%pip install gputil\n",
        "%pip install psutil\n",
        "%pip install humanize"
      ],
      "metadata": {
        "id": "Nodl1Fclc26G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0t5gJ-mHbt4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import einops\n",
        "import pysvelte\n",
        "import tqdm\n",
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from jaxtyping import Float, Int\n",
        "import plotly.express as px\n",
        "import os,sys,humanize,psutil,GPUtil\n",
        "import gc\n",
        "import copy"
      ],
      "metadata": {
        "id": "dFcxIVzU7QA0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mem_report():\n",
        "  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "  \n",
        "  GPUs = GPUtil.getGPUs()\n",
        "  for i, gpu in enumerate(GPUs):\n",
        "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n"
      ],
      "metadata": {
        "id": "q1-IeWPwZ_-T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_large = HookedTransformer.from_pretrained(\n",
        "    \"gpt2-large\",\n",
        "    center_unembed=True,\n",
        "    center_writing_weights=True,\n",
        "    fold_ln=True,\n",
        "    refactor_factored_attn_matrices=True\n",
        ")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqMkXDw9zTPB",
        "outputId": "b6373342-8dbc-4c00-ce8f-ac6c2ff6ee75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-large into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "races = [\"Indian\", \"Polish\"]\n",
        "for race in tqdm.tqdm(races):\n",
        "  text = \"An example of an \" + race + \" food is the\"\n",
        "  for i in range(2):\n",
        "      logits = gpt2_large(text, return_type=\"logits\")\n",
        "      text += gpt2_large.tokenizer.decode(logits[-1, -1].argmax())\n",
        "  print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdd3NS_QPLpi",
        "outputId": "e91551c1-7357-4edd-bd03-78ff9f036bfe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1/2 [00:03<00:03,  3.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example of an Indian food is the dal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example of an Polish food is the \"K\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "races = [\"Indian\", \"Chinese\"]\n",
        "for race in tqdm.tqdm(races):\n",
        "  text =  \"The \" + race + \" have an annual festival called\"\n",
        "  for i in range(15):\n",
        "      logits = gpt2_large(text, return_type=\"logits\")\n",
        "      text += gpt2_large.tokenizer.decode(logits[-1, -1].argmax())\n",
        "  print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkEutj6nPHrr",
        "outputId": "c2d9c4de-1ea2-424d-fe0e-3fd5c23fa684"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Indian have an annual festival called Diwali, which is celebrated on the first day of the Hindu lunar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Chinese have an annual festival called the Spring Festival, which is celebrated on the first day of the lunar new\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Indian people's right\"\n",
        "for i in range(30):\n",
        "      logits = gpt2_large(text, return_type=\"logits\")\n",
        "      text += gpt2_large.tokenizer.decode(logits[-1, -1].argmax())\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0_tu8XN4H_0",
        "outputId": "5b94d6e9-841b-4a27-98c6-1078b82cd22e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian students should be admitted over their peers because they are more likely to be successful in the job market, the country's top education regulator has said.\n",
            "\n",
            "\n",
            "The National Council for Technical Education\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mem_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ba1AxHdnykV",
        "outputId": "40033cff-b716-45b8-e34d-8c4b497fdfc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU RAM Free: 20.4 GB\n",
            "GPU 0 ... Mem Free: 4040MB / 16384MB | Utilization  74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_prompts = [\"The Polish live in the city of\", \"The Indian live in the city of\",\n",
        "                 \"An example of an Indian food is the\", \"The Indian have an annual festival called\",\n",
        "                 \"The Chinese have an annual festival called\"]\n",
        "corrupted_prompts = [\"The Indian live in the city of\", \"The Polish live in the city of\",\n",
        "                     \"An example of an Polish food is the\", \"The Chinese have an annual festival called\",\n",
        "                     \"The Indian have an annual festival called\"]\n",
        "answers = [\"Mumbai\", \"Krakow\", \"dal\", \"Diwali\", \"the Spring Festival\"]"
      ],
      "metadata": {
        "id": "0f8LA2S1UsVC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_logits(prompt, answer_tokens):\n",
        "    prompt_tokens = gpt2_large.to_tokens(prompt, prepend_bos=True)\n",
        "    logits, cache = gpt2_large.run_with_cache(prompt_tokens, return_type=\"logits\")\n",
        "    final_logits = logits[:, -1, :]\n",
        "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
        "    return answer_logits.mean()"
      ],
      "metadata": {
        "id": "jEF7GtRs94Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_caches = []\n",
        "for i in range(5):\n",
        "    clean_prompt = clean_prompts[i]\n",
        "    corrupted_prompt = corrupted_prompts[i]\n",
        "    answer = answers[i]\n",
        "\n",
        "    prompt_tokens = gpt2_large.to_tokens(clean_prompt, prepend_bos=True)\n",
        "    logits, clean_cache = gpt2_large.run_with_cache(prompt_tokens, device=\"cpu\", return_type=\"logits\")\n",
        "    clean_caches.append(clean_cache)"
      ],
      "metadata": {
        "id": "tm022rTqX9Ct"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_list = []\n",
        "\n",
        "def get_mlp_hook(\n",
        "  hook, \n",
        "  pos, \n",
        "  cache):\n",
        "  # patch corrupted component for some position in text with corresponding\n",
        "  # compontent from the clean version\n",
        "  mlp_list.append(cache[hook.name][:, pos, :])\n",
        "  return \n",
        "\n",
        "def get_mlp(layer, position, cache):\n",
        "  gpt2_large.run_with_hooks(\n",
        "        fwd_hooks = [(utils.get_act_name(\"mlp_out\", layer), position, cache,\n",
        "                      get_mlp_hook)], \n",
        "        return_type=\"None\"\n",
        "  )\n",
        "\n",
        "get_mlp()\n",
        "#clean_caches[0].get_neuron_results(layer=1)\n",
        "#print(clean_caches[1].get_neuron_results(1, pos_slice=(2))[0::].shape)\n",
        "#clean_caches[2].get_neuron_results(1, pos_slice=(2))[0::]\n",
        "# 5, 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "dRsMu667AH81",
        "outputId": "5951b02a-2ced-4bae-dda0-9a9fd4817c15"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-cb968e070814>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    def get_mlp(layer, position)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_tokens = gpt2_large.to_tokens(answer, prepend_bos=False)\n",
        "corrupted_tokens = gpt2_large.to_tokens(corrupted_prompt, prepend_bos=True)\n",
        "corrupted_answer_logits = get_output_logits(corrupted_prompt, answer_tokens)\n",
        "clean_answer_logits = get_output_logits(clean_prompt, answer_tokens)"
      ],
      "metadata": {
        "id": "wwl33FWn-8dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_tokens, clean_answer_logits, corrupted_answer_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAwd71vWV_eX",
        "outputId": "8a110c13-a996-48ee-8d23-2118c7ed5fc9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   35, 14246,  7344]], device='cuda:0') tensor(3.1139, device='cuda:0', grad_fn=<MeanBackward0>) tensor(2.1499, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokens = gpt2_large.to_tokens(clean_prompt, prepend_bos=True)\n",
        "logits, clean_cache = gpt2_large.run_with_cache(prompt_tokens, device=\"cpu\", return_type=\"logits\")"
      ],
      "metadata": {
        "id": "7X0akDf0Ky3k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(tensor, renderer=None, **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)"
      ],
      "metadata": {
        "id": "8g23ym39_ljq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_residual_component(\n",
        "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
        "    hook, \n",
        "    pos, \n",
        "    clean_cache):\n",
        "    # patch corrupted component for some position in text with corresponding\n",
        "    # compontent from the clean version\n",
        "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
        "    return corrupted_residual_component\n",
        "\n",
        "def normalized_patched_logit(patched_logits):\n",
        "    # 0 -> no change between corrupted before and after aptch.\n",
        "    # < 1 -> logits before patch were larger -> dal more probable before patch\n",
        "    # > 1 -> logits after patch were larger -> dal more probable after patch\n",
        "    return (patched_logits - corrupted_answer_logits)/(clean_answer_logits - corrupted_answer_logits)\n"
      ],
      "metadata": {
        "id": "5eDdo2kmtj4m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def runPatchedMLP(layer, position):\n",
        "    hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)\n",
        "    patched_MLP_logits = gpt2_large.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.get_act_name(\"mlp_out\", layer), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "    return normalized_patched_logit(patched_MLP_logits[:, -1, :].gather(dim=-1, index=answer_tokens).mean())\n",
        "  \n",
        "def runPatchedResids(layer, position):\n",
        "    hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)\n",
        "    patched_resid_stream_logits = gpt2_large.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.get_act_name(\"resid_pre\", layer), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "    return normalized_patched_logit(patched_resid_stream_logits[:, -1, :].gather(dim=-1, index=answer_tokens).mean())\n",
        "\n",
        "def runPatchedAttention(layer, position):\n",
        "    hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)\n",
        "    patched_attn_logits = gpt2_large.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.get_act_name(\"attn_out\", layer), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "    return normalized_patched_logit(patched_attn_logits[:, -1, :].gather(dim=-1, index=answer_tokens).mean())\n",
        "\n",
        "def patch_head_vector(\n",
        "    corrupted_head_vector: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
        "    hook, \n",
        "    head_index, \n",
        "    clean_cache):\n",
        "    corrupted_head_vector[:, :, head_index, :] = clean_cache[hook.name][:, :, head_index, :]\n",
        "    return corrupted_head_vector\n",
        "\n",
        "def runPatchedHeads(layer, head_index):\n",
        "    hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=clean_cache)\n",
        "    patched_head_logits = gpt2_large.run_with_hooks(\n",
        "        corrupted_tokens, \n",
        "        fwd_hooks = [(utils.get_act_name(\"z\", layer, \"attn\"), \n",
        "            hook_fn)], \n",
        "        return_type=\"logits\"\n",
        "    )\n",
        "    return normalized_patched_logit(patched_head_logits[:, -1, :].gather(dim=-1, index=answer_tokens).mean())\n"
      ],
      "metadata": {
        "id": "wybB_DtGzc5F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MLPPatching():\n",
        "  patched_mlp_logits = torch.zeros(gpt2_large.cfg.n_layers, corrupted_tokens.shape[1], device=\"cpu\", dtype=torch.float32)\n",
        "  for layer in tqdm.tqdm(range(gpt2_large.cfg.n_layers)):\n",
        "    for position in range(corrupted_tokens.shape[1]):\n",
        "      # Define a hook function to replacing an activation with its cleaned counterpart\n",
        "      # evaluate patched model on corrupted prompt\n",
        "      patched_mlp_logits[layer, position] = copy.copy(runPatchedMLP(layer, position))\n",
        "  prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(gpt2_large.to_str_tokens(corrupted_tokens[0]))]\n",
        "  imshow(patched_mlp_logits, x=prompt_position_labels, title=\"Logit From Patched MLP Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})    \n",
        "\n",
        "def AttnPatching():\n",
        "  patched_attn_logits = torch.zeros(gpt2_large.cfg.n_layers, corrupted_tokens.shape[1], device=\"cpu\", dtype=torch.float32)\n",
        "  for layer in tqdm.tqdm(range(gpt2_large.cfg.n_layers)):\n",
        "    for position in range(corrupted_tokens.shape[1]):\n",
        "      patched_attn_logits[layer, position] = copy.copy(runPatchedAttention(layer, position))\n",
        "  prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(gpt2_large.to_str_tokens(corrupted_tokens[0]))]\n",
        "  imshow(patched_attn_logits, x=prompt_position_labels, title=\"Logit From Patched Attention Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})    \n",
        "\n",
        "def ResidStreamPatching():\n",
        "  patched_resid_logits = torch.zeros(gpt2_large.cfg.n_layers, corrupted_tokens.shape[1], device=\"cpu\", dtype=torch.float32)\n",
        "  for layer in tqdm.tqdm(range(gpt2_large.cfg.n_layers)):\n",
        "    for position in range(corrupted_tokens.shape[1]):\n",
        "      patched_resid_logits[layer, position] = copy.copy(runPatchedResids(layer, position))\n",
        "  prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(gpt2_large.to_str_tokens(corrupted_tokens[0]))]\n",
        "  imshow(patched_resid_logits, x=prompt_position_labels, title=\"Logit From Patched Residual Stream\", labels={\"x\":\"Position\", \"y\":\"Layer\"})    \n",
        "\n",
        "\n",
        "def HeadPatching():\n",
        "  patched_head_logits = torch.zeros(gpt2_large.cfg.n_layers, gpt2_large.cfg.n_heads, device=\"cpu\", dtype=torch.float32)\n",
        "  for layer in range(gpt2_large.cfg.n_layers):\n",
        "    for head_index in range(gpt2_large.cfg.n_heads):\n",
        "      patched_head_logits[layer, head_index] = copy.copy(runPatchedHeads(layer, head_index))\n",
        "      prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(gpt2_large.to_str_tokens(corrupted_tokens[0]))]\n",
        "  imshow(patched_head_logits, title=\"Logit From Patched Head output\", labels={\"x\":\"Head\", \"y\":\"Layer\"})    \n",
        "\n",
        "MLPPatching()\n",
        "#AttnPatching()\n",
        "#ResidStreamPatching()\n",
        "#HeadPatching()\n",
        "# Clean: Polish city is Warsaw. 0, 1, 2, 3, 4, 5, 6, 8, 9\n",
        "# Clean: Indian city is Mumbai. 0, 1, 2, 18\n",
        "# Clean: Indian food is dal. 0, 1, 2, 18\n",
        "# Clean: Indian festival is Diwali. 0, 1, 2, 3, 5, 7, 18\n",
        "# Clean: Chinese festival is the spring festival. 0, 1, 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "eWxkdp-HZztZ",
        "outputId": "e578474a-2717-4a6d-ff30-fccb073e2045"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [00:29<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"ce95c894-5b64-479b-b2ad-1a9b3337e856\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ce95c894-5b64-479b-b2ad-1a9b3337e856\")) {                    Plotly.newPlot(                        \"ce95c894-5b64-479b-b2ad-1a9b3337e856\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"<|endoftext|>_0\",\"The_1\",\" Indian_2\",\" have_3\",\" an_4\",\" annual_5\",\" festival_6\",\" called_7\"],\"z\":[[0.0,0.0,0.9424067139625549,0.0010798209114000201,0.0004860192129854113,0.0002180599549319595,-0.000981020275503397,0.0002285388036398217],[0.0,0.0,0.21209798753261566,0.0003472991520538926,0.0016885923687368631,0.0010119578801095486,-0.01050080917775631,0.0035358646418899298],[0.0,0.0,0.07019684463739395,0.0012255269102752209,0.0011077645467594266,-0.0008043767884373665,-0.0013013738207519054,0.0006651576841250062],[0.0,0.0,0.016381941735744476,0.004906598478555679,0.0009166502277366817,-0.0006287312135100365,-0.001238999655470252,0.0014231281820684671],[0.0,0.0,-0.021373368799686432,0.009014309383928776,-0.0008692458504810929,-0.0011297202436253428,-0.01764938421547413,0.0018228215631097555],[0.0,0.0,0.046679798513650894,0.007430505473166704,-0.0006551778642460704,-0.0005498903337866068,0.0121380053460598,0.003126690397039056],[0.0,0.0,-0.018618427217006683,-0.003235470736399293,-0.0014111523050814867,-9.480867447564378e-05,-0.006196994334459305,0.012011759914457798],[0.0,0.0,-0.01395783293992281,0.014543650671839714,0.0011037725489586592,0.003998930100351572,-0.01343089621514082,-0.0013098566560074687],[0.0,0.0,-0.03373542055487633,-0.0049684736877679825,0.0008812216692604125,0.0013682389399036765,5.688520468538627e-05,0.006309268064796925],[0.0,0.0,0.041294168680906296,-0.008221409283578396,0.002205050317570567,-0.0005044819554314017,0.01284058764576912,0.0005269366665743291],[0.0,0.0,0.012662946246564388,-0.0027709084097296,0.0049704695120453835,-0.001653662882745266,-0.002045372501015663,0.0009261310915462673],[0.0,0.0,0.01910843886435032,0.016538625583052635,-0.004426068160682917,-0.0002654643030837178,0.01194489561021328,0.00042314609163440764],[0.0,0.0,0.0069300150498747826,-0.013039685785770416,-0.0020543544087558985,0.001412150333635509,-0.00937358383089304,-0.0015598522732034326],[0.0,0.0,-0.020062513649463654,-0.0544261671602726,-0.000494502077344805,-0.0039640008471906185,0.003642649156972766,-0.012456363067030907],[0.0,0.0,-0.010888527147471905,-0.0024191183038055897,0.001231514848768711,0.00511118583381176,0.0002988968335557729,-0.016026658937335014],[0.0,0.0,0.026830356568098068,-0.04195583239197731,-0.0005648601218126714,0.0039640008471906185,0.014012722298502922,0.012363550253212452],[0.0,0.0,0.018297575414180756,0.003225490916520357,0.00010179457603953779,-0.0013996755005791783,-0.0024565428029745817,-0.01653163880109787],[0.0,0.0,0.031283870339393616,-0.00551986088976264,0.0028247996233403683,-0.0004411098489072174,0.021393828094005585,0.015591536648571491],[0.0,0.0,0.0489741712808609,-0.01447478961199522,0.002044873544946313,-0.005585728213191032,-0.006640599109232426,-0.02577398717403412],[0.0,0.0,0.04002123698592186,0.007716927211731672,0.004602711647748947,0.0012963839108124375,0.0006606667884625494,0.003526383778080344],[0.0,0.0,0.030024411156773567,-0.005855683237314224,-0.0031865695491433144,-0.0013567620189860463,-0.0038138036616146564,0.0013153456384316087],[0.0,0.0,0.006122145336121321,0.0017609463538974524,7.584693958051503e-05,-0.0020493643824011087,-0.0041112033650279045,-0.031947530806064606],[0.0,0.0,0.008928981609642506,0.013883981853723526,0.0012908949283882976,-0.003743944689631462,0.00464662304148078,0.08269761502742767],[0.0,0.0,-0.0036421502009034157,-0.0029071334283798933,0.0011900983517989516,0.0028761958237737417,0.0009151532431133091,0.007531800772994757],[0.0,0.0,-0.015704309567809105,0.0011237323051318526,9.580665937392041e-05,-0.002185090444982052,0.0014929871540516615,0.000664658728055656],[0.0,0.0,0.015485250391066074,-0.00011127544712508097,-0.0008178495918400586,0.0018522621830925345,0.0014041663380339742,0.05776243656873703],[0.0,0.0,0.0014630475779995322,0.000320353516144678,-0.0026237054262310266,0.0013382993638515472,-0.003438062034547329,0.04423273727297783],[0.0,0.0,0.004870172124356031,-0.00019660325779113919,-0.0028871737886220217,0.0035458444617688656,0.001002477016299963,0.01852411776781082],[0.0,0.0,0.0015304116532206535,-0.0011786215472966433,-0.0012958849547430873,-0.00024899753043428063,-0.0047154841013252735,0.1305924654006958],[0.0,0.0,0.007991872727870941,0.0016940813511610031,0.00041366522782482207,0.006006379146128893,0.0034320740960538387,0.24036993086338043],[0.0,0.0,0.00038921457598917186,-0.0009141552145592868,0.0009430968202650547,0.004337247461080551,0.002454546745866537,0.14417456090450287],[0.0,0.0,0.0013363033067435026,0.0004251420614309609,0.0006147593958303332,0.0036256834864616394,9.780263644643128e-05,0.07916025817394257],[0.0,0.0,0.002188084414228797,0.0005668560625053942,-0.0012185409432277083,0.0018013648223131895,0.00060627656057477,0.23073886334896088],[0.0,0.0,0.0023811948485672474,-0.000466558471089229,0.00011876034113811329,-0.001039901515468955,-0.0025748040061444044,0.1442548930644989],[0.0,0.0,0.0008672499097883701,-0.0014301140327006578,0.00022155290935188532,0.001218041987158358,-0.00026945624267682433,0.028283923864364624],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.05814217031002045]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit From Patched MLP Layer\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ce95c894-5b64-479b-b2ad-1a9b3337e856');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_residual_component(\n",
        "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
        "    hook, \n",
        "    pos, \n",
        "    clean_cache):\n",
        "    # patch corrupted component for some position in text with corresponding\n",
        "    # compontent from the clean version\n",
        "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
        "    return corrupted_residual_component\n",
        "    \n",
        "def MLPPatching():\n",
        "  patched_mlp_logits = torch.zeros(gpt2_large.cfg.n_layers, corrupted_tokens.shape[1], device=\"cpu\", dtype=torch.float32)\n",
        "  for layer in tqdm.tqdm(range(gpt2_large.cfg.n_layers)):\n",
        "    for position in range(corrupted_tokens.shape[1]):\n",
        "      # Define a hook function to replacing an activation with its cleaned counterpart\n",
        "      # evaluate patched model on corrupted prompt\n",
        "      patched_mlp_logits[layer, position] = copy.copy(runPatchedMLP(layer, position))\n",
        "  prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(gpt2_large.to_str_tokens(corrupted_tokens[0]))]\n",
        "  imshow(patched_mlp_logits, x=prompt_position_labels, title=\"Logit From Patched MLP Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})    \n",
        "\n",
        "def zoomIntoMLP(layer, position):\n",
        "    hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)\n",
        "    patched_MLP_logits = gpt2_large.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.get_act_name(\"mlp_out\", layer), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "    return normalized_patched_logit(patched_MLP_logits[:, -1, :].gather(dim=-1, index=answer_tokens))\n",
        "# MLP layers 0, 1, 2, 4, 5, 18\n",
        "layers = [0, 1, 2, 4, 5, 18]\n",
        "for layer in layers:\n"
      ],
      "metadata": {
        "id": "5Gks6aZcIdFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "# [n_layers, position]\n",
        "patched_mlp_logits = torch.zeros(gpt2_large.cfg.n_layers, corrupted_tokens.shape[1], device=\"cpu\", dtype=torch.float32)\n",
        "for layer in range(gpt2_large.cfg.n_layers):\n",
        "  for position in range(corrupted_tokens.shape[1]):\n",
        "    print(layer, position)\n",
        "    # Define a hook function to  replacing an activation with its cleaned counterpart\n",
        "    # evaluate patched model on corrupted prompt\n",
        "    patched_mlp_logits[layer, position] = copy.copy(runPatched(layer, position))\n",
        "    print(patched_mlp_logits.get_device()) \n",
        "\n",
        "prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(gpt2_large.to_str_tokens(corrupted_tokens[0]))]\n",
        "imshow(patched_mlp_logits, x=prompt_position_labels, title=\"Logit From Patched MLP Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})"
      ],
      "metadata": {
        "id": "mPTHbSMZr4Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8RXoO_gljnuF"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}
